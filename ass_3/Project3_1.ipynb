{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3\n",
    "\n",
    "### Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Members: Lars Olav Thorbjørnsen, Stein Are Årsnes og Sanjai Vijayaratnam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abstract\n",
    "\n",
    "Data preprocessing and feature selection are essential steps in machine learning(ML) to ensure model accuracy and reliability. In this task, we take the raw log data, containing shear wave velocity (Vs), density (DEN), neutron porosity (NEU) and compressional wave velocity (Vp), clean it and prepare it for analysis. We start with checking that all data is of the float type before moving on to missing values, duplicates, outliers and conducting a correlation analysis. After removing missing values, duplicates and most of the outliers, we choose too keep all features since they are all above the 0.5 threshold. In the end we do some filtering and display the results. We have now laid a strong foundation for ML modeling, improving the reliability of Vp estimation from the log data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Effective data preprocessing and feature selection are important in building accurate ML models. In this project, raw data logs with attributes like Vs, DEN and NEU needs to be cleaned and prepared before being used to estimate Vp. Raw data could be incomplete, redundant or noisy, by data preprocessing these issues can be fixed [1]. This task aimed to enhance data quality and select meaningful predictors to ensure the effectiveness of ML models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "language_info": {
     "name": "polyglot-notebook"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd  \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "df = pd.read_excel('ProjectData2024.xlsx')\n",
    "df.info()\n",
    "df.head(), df.tail(20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start of with importing the libraries we need for this task and then import the data file. Now that we have the data file we take a quick look at it and discover that the file has many missing values. We also see that all the values are of type float so any converting will not be needed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "language_info": {
     "name": "polyglot-notebook"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "df.replace(0, np.nan, inplace=True)\n",
    "df.replace('', np.nan,inplace=True)\n",
    "df.info()\n",
    "df.head(), df.tail(20)\n",
    "\n",
    "df.dropna(inplace=True) #Removes/drops all Nan values in df\n",
    "miss_data= df.isnull().sum() #checks if there is any missing values\n",
    "print(miss_data)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method we have used to remove missing values is to replace missing or empty values with NaN and then removing all NaN values. As you can see we now have 0 missing values. We move on to duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "language_info": {
     "name": "polyglot-notebook"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "dups= df.duplicated()\n",
    "print(dups.any())\n",
    "print(df[dups])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check if the data frame contains any duplicates, it does as the dups.any() function returns true. We then print set duplicates and discover quite allot of values. Now starts the process of removing them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "language_info": {
     "name": "polyglot-notebook"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace = True)\n",
    "\n",
    "dups2= df.duplicated()\n",
    "print(dups2.any())\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After removing the duplicates we again check if there are any and this time it returns false, we have removed all duplicates. We move on to outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "language_info": {
     "name": "polyglot-notebook"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "df.shape\n",
    "plt.boxplot(df,widths=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By plotting this boxplot we clearly sees that there is many outliers in our data frame. We can also look at a single column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "language_info": {
     "name": "polyglot-notebook"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "def plot_boxplot(df, ft):\n",
    "    df.boxplot(column=[ft])\n",
    "    plt.grid(False)\n",
    "    plt.show()\n",
    "plot_boxplot(df, \"Vp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a function for plotting a boxplot of a specific column in the data frame. As shown in the boxplot we see many outliers in the Vp column. Now starts the task of removing set outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "language_info": {
     "name": "polyglot-notebook"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "def outliers(df, ft):\n",
    "    Q1 = df[ft].quantile(0.25)\n",
    "    Q3 = df[ft].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    UB = Q3 + 1.5*IQR\n",
    "    LB = Q1-1.5*IQR\n",
    "    ls = df.index[ (df[ft] < LB) | (df[ft] > UB)]\n",
    "    return ls\n",
    "\n",
    "index_list = []\n",
    "for f in ['Vs', 'DEN', 'NEU', 'Vp']:\n",
    "    index_list.extend(outliers(df, f))\n",
    "\n",
    "def remove(df, ls):\n",
    "    ls = sorted(set(ls))\n",
    "    df = df.drop(ls)\n",
    "    return df\n",
    "\n",
    "df_cleaned = remove(df, index_list)\n",
    "\n",
    "print(\"New shape\", df_cleaned.shape)\n",
    "df_cleaned.info()\n",
    "plt.boxplot(df_cleaned,widths=0.5)\n",
    "df_cleaned.to_excel('CleanOutlier1.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a function for locating all outliers one column at a time using the IQR method. Then we create a for loop that loops for all columns and inserts the outliers for each column into a list. We also create a function that takes a list and a data frame, this function we use to remove all outliers from our data frame using the list of outliers. We now see that almost all of the outliers are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "language_info": {
     "name": "polyglot-notebook"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "plot_boxplot(df_cleaned, \"Vp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how the Vp column now looks after removing outliers. Much better than before. We will now look at correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "language_info": {
     "name": "polyglot-notebook"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    " \n",
    "df2 = pd.read_excel('CleanOutlier1.xlsx').astype(float)\n",
    "print(df2.head()) \n",
    "sns.heatmap(df2.corr(), annot = True, fmt= '0.4')\n",
    "CorrelationData=df2.corr()\n",
    "\n",
    "CorrelationData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We display the correlation data in numbers and in a heat map, from this data we see that Vs, DEN and NEU all have a big correlation with Vp. Both Vs and DEN have a positive correlation of almost the exact same size, NEU has a negative correlation which means that when one gets larger the other grows smaller [2]. NEUs correlation is the greatest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "language_info": {
     "name": "polyglot-notebook"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "CorrelationData['Vp'][abs(CorrelationData['Vp']) > 0.5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the project 0.5 is a good threshold to use when selecting features, we see that all our features pass this threshold so we will not drop any features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "language_info": {
     "name": "polyglot-notebook"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "df3 = pd.read_excel('CleanOutlier1.xlsx')\n",
    "df3.info()\n",
    "plt.plot(df3)\n",
    "plt.ylim(0,5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the cleaned data and we see that the data contains some noise, to get clearer data we can apply some filtering/smoothening. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "language_info": {
     "name": "polyglot-notebook"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(df3['Vp'])\n",
    "dfx1 = df3['Vp'].rolling(window =15).mean().plot(figsize=(10,6))\n",
    "plt.show()\n",
    "\n",
    "plt.plot(df3['Vs'])\n",
    "dfx1 = df3['Vs'].rolling(window =15).mean().plot(figsize=(10,6))\n",
    "plt.show()\n",
    "\n",
    "plt.plot(df3['DEN'])\n",
    "dfx1 = df3['DEN'].rolling(window =15).mean().plot(figsize=(10,6))\n",
    "plt.show()\n",
    "\n",
    "plt.plot(df3['NEU'])\n",
    "dfx1 = df3['NEU'].rolling(window =15).mean().plot(figsize=(10,6))\n",
    "plt.show()\n",
    "\n",
    "dataF = df3.rolling(window =15).mean()\n",
    "dataF.dropna(inplace=True) #Some values went missing after filtering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graphs show the data with and without the filter, orange being with filter. We see that there is less noise/spikes and the data is more consistent.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "language_info": {
     "name": "polyglot-notebook"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(dataF)\n",
    "plt.ylim(0,5)\n",
    "plt.show()\n",
    "dataF.to_excel('CleanedFeatureSelectedFiltered.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again the graph with all the data now shows much less spikes/noise. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion \n",
    "The data preprocessing and feature selection steps were key in enhancing the data quality and model readiness. We also saw how big a difference filtering made in terms of noise and spikes. By handling data inconsistencies and focusing on high-correlation features we have improved the accuracy potential of our Vp estimation models. This shows how important data preprocessing is to create a reliable foundation for the ML models to follow. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection\n",
    "Reflecting over this data preprocessing task, we got a new understanding of how important data integrity is for ML projects. We also learned about how correlation works in terms of feature selection with both negative and positive correlation. Another observation is how big a difference filtering makes, just looking at graphs really shows how important filtering can be when reducing noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "[1]:Amit Kumar Tyagi, Ajith Abraham,\"2.5.1 Data preprocessing\", Data science for Genomics, 2023\n",
    "\n",
    "\n",
    "[2]:JOVE 1.13 Correlations, 09.11.2024, https://www.jove.com/science-education/11030/correlation-correlation-coefficient-positive-negative-correlation "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".NET (C#)",
   "language": "C#",
   "name": ".net-csharp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
